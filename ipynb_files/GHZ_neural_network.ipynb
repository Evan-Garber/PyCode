{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fields = ['input1', 'input2','output1', 'output2']\n",
    "rows = []\n",
    "h_target = torch.tensor([[1.,1.],[1.,-1]] / np.sqrt(2), dtype=torch.complex128)\n",
    "x_target = torch.tensor([[0.,1.],[1.,0.]], dtype=torch.complex128)\n",
    "z_target = torch.tensor([[1.,0.],[0.,-1.]], dtype=torch.complex128)\n",
    "\n",
    "for i in range(8):\n",
    "    phi = np.random.rand() * 2*np.pi\n",
    "    theta = np.random.rand() * np.pi\n",
    "    \n",
    "    input_qubit = torch.tensor([np.sin(theta), np.cos(theta) * np.exp(1j * phi)])\n",
    "    output = h_target @ input_qubit\n",
    "\n",
    "    rows.append([np.sin(theta), np.cos(theta) * np.exp(1j * phi), output[0].item(), output[1].item()])\n",
    "\n",
    "filename = 'qubit.csv'\n",
    "\n",
    "with open(filename, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields)\n",
    "    csvwriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e1a404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input1</th>\n",
       "      <th>input2</th>\n",
       "      <th>output1</th>\n",
       "      <th>output2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.209295</td>\n",
       "      <td>(0.05365696150212711+0.9763792946859188j)</td>\n",
       "      <td>(0.1859351258582564+0.6904044202825516j)</td>\n",
       "      <td>(0.11005272318621719-0.6904044202825516j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.482308</td>\n",
       "      <td>(0.8446007558876549+0.23243970403815695j)</td>\n",
       "      <td>(0.9382664287419215+0.1643596909423749j)</td>\n",
       "      <td>(-0.25617941502496777-0.1643596909423749j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997045</td>\n",
       "      <td>(-0.01367213808107157-0.07559852921280168j)</td>\n",
       "      <td>(0.6953493364790899-0.05345623265410137j)</td>\n",
       "      <td>(0.714684659579979+0.05345623265410137j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999986</td>\n",
       "      <td>(-0.005009294030824517+0.0017058662841566673j)</td>\n",
       "      <td>(0.7035547747811988+0.0012062296173246773j)</td>\n",
       "      <td>(0.7106389863375053-0.0012062296173246773j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.913896</td>\n",
       "      <td>(-0.1610944384852178+0.37261621264195866j)</td>\n",
       "      <td>(0.532311027581392+0.2634794507391775j)</td>\n",
       "      <td>(0.7601329673100654-0.2634794507391775j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.492788</td>\n",
       "      <td>(-0.8637682975895715-0.10518904658471073j)</td>\n",
       "      <td>(-0.262322985720642-0.0743798881465966j)</td>\n",
       "      <td>(0.9592298554784495+0.0743798881465966j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.509083</td>\n",
       "      <td>(-0.8505860230452449-0.13167204335771016j)</td>\n",
       "      <td>(-0.241478832880681-0.09310619475092595j)</td>\n",
       "      <td>(0.9614314568748983+0.09310619475092595j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.526200</td>\n",
       "      <td>(-0.8502077651500879+0.016118811153745045j)</td>\n",
       "      <td>(-0.22910775194156324+0.011397720671478479j)</td>\n",
       "      <td>(0.9732676003686103-0.011397720671478479j)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     input1                                          input2  \\\n",
       "0  0.209295       (0.05365696150212711+0.9763792946859188j)   \n",
       "1  0.482308       (0.8446007558876549+0.23243970403815695j)   \n",
       "2  0.997045     (-0.01367213808107157-0.07559852921280168j)   \n",
       "3  0.999986  (-0.005009294030824517+0.0017058662841566673j)   \n",
       "4  0.913896      (-0.1610944384852178+0.37261621264195866j)   \n",
       "5  0.492788      (-0.8637682975895715-0.10518904658471073j)   \n",
       "6  0.509083      (-0.8505860230452449-0.13167204335771016j)   \n",
       "7  0.526200     (-0.8502077651500879+0.016118811153745045j)   \n",
       "\n",
       "                                        output1  \\\n",
       "0      (0.1859351258582564+0.6904044202825516j)   \n",
       "1      (0.9382664287419215+0.1643596909423749j)   \n",
       "2     (0.6953493364790899-0.05345623265410137j)   \n",
       "3   (0.7035547747811988+0.0012062296173246773j)   \n",
       "4       (0.532311027581392+0.2634794507391775j)   \n",
       "5      (-0.262322985720642-0.0743798881465966j)   \n",
       "6     (-0.241478832880681-0.09310619475092595j)   \n",
       "7  (-0.22910775194156324+0.011397720671478479j)   \n",
       "\n",
       "                                       output2  \n",
       "0    (0.11005272318621719-0.6904044202825516j)  \n",
       "1   (-0.25617941502496777-0.1643596909423749j)  \n",
       "2     (0.714684659579979+0.05345623265410137j)  \n",
       "3  (0.7106389863375053-0.0012062296173246773j)  \n",
       "4     (0.7601329673100654-0.2634794507391775j)  \n",
       "5     (0.9592298554784495+0.0743798881465966j)  \n",
       "6    (0.9614314568748983+0.09310619475092595j)  \n",
       "7   (0.9732676003686103-0.011397720671478479j)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('qubit.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6968175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.220399449552511\n",
      "epoch: 500, loss: 8.78896955214259e-12\n",
      "tensor([-0.1351+0.6941j, -0.1351+0.6941j], dtype=torch.complex128,\n",
      "       grad_fn=<MvBackward0>)\n",
      "tensor([-0.1351+0.6941j,  0.0000+0.0000j,  0.0000+0.0000j, -0.1351+0.6941j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n",
      "tensor([-0.1351+0.6941j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, -0.1351+0.6941j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n",
      "tensor([-0.1351+0.6941j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, -0.1351+0.6941j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "#There are 3 versions to this code: \n",
    "# with a global phase\n",
    "# without a global phase\n",
    "# without a global phase and a different matrix for U\n",
    "\n",
    "#This version has a global phase, but I'm not really sure how to get rid of it.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "import pandas as pd\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "n_ghz = 4\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_epochs = 501\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = pd.read_csv('qubit.csv')\n",
    "        self.df['input1'] = self.df['input1'].astype(complex)\n",
    "        self.df['input2'] = self.df['input2'].astype(complex)\n",
    "        self.df['output1'] = self.df['output1'].astype(complex)\n",
    "        self.df['output2'] = self.df['output2'].astype(complex)\n",
    "\n",
    "        dataset = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "            dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    "            \n",
    "        self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    "        \n",
    "        self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "def elements_to_matrix(matrix_entries: list):\n",
    "    return torch.stack([torch.stack([value for value in row]) for row in matrix_entries]).squeeze()\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        self.θ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.α = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.β = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.ϕ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "      \n",
    "    def forward(self, x):\n",
    "        θ = self.θ\n",
    "        α = self.α\n",
    "        β = self.β\n",
    "        ϕ = self.ϕ\n",
    "        \n",
    "        U = torch.exp(1j * ϕ / 2) * elements_to_matrix(\n",
    "            [[torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "             [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]])\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            return U.matmul(x)\n",
    "        else:\n",
    "            return torch.einsum('ij,bj->bi', U, x)\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "c_not = torch.tensor([[1,0,0,0],\n",
    "                      [0,1,0,0],\n",
    "                      [0,0,0,1],\n",
    "                      [0,0,1,0]], dtype=torch.complex128)\n",
    "\n",
    "i = torch.tensor([[1,0],\n",
    "                  [0,1]], dtype=torch.complex128)\n",
    "\n",
    "def quantum_infidelity_batched(state_batch, target_state_batch):\n",
    "\n",
    "    return torch.stack([torch.abs(1 - torch.abs(torch.dot(target_state.conj(), state))**2)\n",
    "                        for state, target_state in zip(state_batch, target_state_batch)]).mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = quantum_infidelity_batched(outputs, labels) \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 500 == 0:\n",
    "        \n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "    \n",
    "model.eval()\n",
    "\n",
    "q0 = model(torch.tensor([1,0],dtype=torch.complex128, requires_grad=False))\n",
    "q1 = torch.tensor([1,0],dtype=torch.complex128, requires_grad=False)\n",
    "print(q0)\n",
    "\n",
    "for n in range(n_ghz-1):\n",
    "    q0 = torch.kron(q0,q1)\n",
    "    q0 = q0 @ c_not\n",
    "    c_not = torch.kron(i,c_not)\n",
    "    print(q0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40d579b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.6662465220462848\n",
      "epoch: 500, loss: 6.132699258387664e-05\n",
      "tensor([0.7071+0.0089j, 0.7070+0.0052j], dtype=torch.complex128,\n",
      "       grad_fn=<MvBackward0>)\n",
      "tensor([0.7071+0.0089j, 0.0000+0.0000j, 0.0000+0.0000j, 0.7070+0.0052j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.7071+0.0089j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,\n",
      "        0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.7070+0.0052j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.7071+0.0089j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,\n",
      "        0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,\n",
      "        0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,\n",
      "        0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.7070+0.0052j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "#This version has no global phase.\n",
    "#One problem I encountered is that whenever I run it, the output is always [0+1j, 0+1j] / sqrt(2)\n",
    "#I found a way to fix it, which I explain below\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "import pandas as pd\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "n_ghz = 4\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_epochs = 501\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = pd.read_csv('qubit.csv')\n",
    "        self.df['input1'] = self.df['input1'].astype(complex)\n",
    "        self.df['input2'] = self.df['input2'].astype(complex)\n",
    "        self.df['output1'] = self.df['output1'].astype(complex)\n",
    "        self.df['output2'] = self.df['output2'].astype(complex)\n",
    "\n",
    "        dataset = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "            dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    "            \n",
    "        self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    "        \n",
    "        self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "def elements_to_matrix(matrix_entries: list):\n",
    "    return torch.stack([torch.stack([value for value in row]) for row in matrix_entries]).squeeze()\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        self.θ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.α = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.β = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.ϕ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "      \n",
    "    def forward(self, x):\n",
    "        θ = self.θ\n",
    "        α = self.α\n",
    "        β = self.β\n",
    "        ϕ = self.ϕ\n",
    "        \n",
    "        y = torch.tensor([[0,-1j],[1j,0]], dtype=torch.complex128) \n",
    "        \n",
    "        U = elements_to_matrix(\n",
    "            [[torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "             [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]])\n",
    "        \n",
    "        U = U @ y\n",
    "        #For whatever reason, if I don't multiply by y, \n",
    "        #the output is always [0+1j, 0+1j] / sqrt(2) instead of [1+0j, 1+0j] / sqrt2\n",
    "        #However, even after multiplying by y, the output is occasionally negative\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            return U.matmul(x)\n",
    "        else:\n",
    "            return torch.einsum('ij,bj->bi', U, x)\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "c_not = torch.tensor([[1,0,0,0],\n",
    "                      [0,1,0,0],\n",
    "                      [0,0,0,1],\n",
    "                      [0,0,1,0]], dtype=torch.complex128)\n",
    "\n",
    "i = torch.tensor([[1,0],\n",
    "                  [0,1]], dtype=torch.complex128)\n",
    "\n",
    "def quantum_infidelity_batched(state_batch, target_state_batch):\n",
    "\n",
    "    return torch.stack([torch.abs(1 - torch.abs(torch.dot(target_state.conj(), state))**2)\n",
    "                        for state, target_state in zip(state_batch, target_state_batch)]).mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = quantum_infidelity_batched(outputs, labels) \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 500 == 0:\n",
    "        \n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "\n",
    "model.eval()\n",
    "\n",
    "q0 = model(torch.tensor([1,0],dtype=torch.complex128, requires_grad=False))\n",
    "q1 = torch.tensor([1,0],dtype=torch.complex128, requires_grad=False)\n",
    "print(q0)\n",
    "\n",
    "for n in range(n_ghz-1):\n",
    "    q0 = torch.kron(q0,q1)\n",
    "    q0 = q0 @ c_not\n",
    "    c_not = torch.kron(i,c_not)\n",
    "    print(q0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f822252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.8291501279779602\n",
      "epoch: 500, loss: 0.009444190380199613\n",
      "epoch: 1000, loss: 0.008927105457153472\n",
      "epoch: 1500, loss: 0.001731836467471215\n",
      "epoch: 2000, loss: 0.0038628578503230315\n",
      "epoch: 2500, loss: 0.0007825601287514417\n",
      "epoch: 3000, loss: 0.0004641278881263311\n",
      "tensor([0.6660+0.0000j, 0.7478-0.0201j], dtype=torch.complex128,\n",
      "       grad_fn=<MvBackward0>)\n",
      "tensor([0.6660+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.7478-0.0201j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.6660+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,\n",
      "        0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.7478-0.0201j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.6660+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,\n",
      "        0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,\n",
      "        0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,\n",
      "        0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.7478-0.0201j],\n",
      "       dtype=torch.complex128, grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "#This version uses a different matrix for U\n",
    "#It doesn't have any problems with being negative or having some rotation, but it isn't as accurate\n",
    "#It usually only gets so precise and takes considerably more epochs\n",
    "#It's not great, but it's good enough\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "import pandas as pd\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "n_ghz = 4\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_epochs = 3001\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = pd.read_csv('qubit.csv')\n",
    "        self.df['input1'] = self.df['input1'].astype(complex)\n",
    "        self.df['input2'] = self.df['input2'].astype(complex)\n",
    "        self.df['output1'] = self.df['output1'].astype(complex)\n",
    "        self.df['output2'] = self.df['output2'].astype(complex)\n",
    "\n",
    "        dataset = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "            dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    "            \n",
    "        self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    "        \n",
    "        self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "def elements_to_matrix(matrix_entries: list):\n",
    "    return torch.stack([torch.stack([value for value in row]) for row in matrix_entries]).squeeze()\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        self.θ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.α = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.β = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.ϕ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "      \n",
    "    def forward(self, x):\n",
    "        θ = self.θ\n",
    "        α = self.α\n",
    "        β = self.β\n",
    "        ϕ = self.ϕ\n",
    "     \n",
    "        U = elements_to_matrix([[α,β],\n",
    "                                [-torch.exp(1j * ϕ) * β.conj(),torch.exp(1j * ϕ) * α.conj()]])\n",
    "\n",
    "        if len(x.shape) == 1:\n",
    "            return U.matmul(x)\n",
    "        else:\n",
    "            return torch.einsum('ij,bj->bi', U, x)\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "c_not = torch.tensor([[1,0,0,0],\n",
    "                      [0,1,0,0],\n",
    "                      [0,0,0,1],\n",
    "                      [0,0,1,0]], dtype=torch.complex128)\n",
    "\n",
    "i = torch.tensor([[1,0],\n",
    "                  [0,1]], dtype=torch.complex128)\n",
    "\n",
    "def quantum_infidelity_batched(state_batch, target_state_batch):\n",
    "\n",
    "    return torch.stack([torch.abs(1 - torch.abs(torch.dot(target_state.conj(), state))**2)\n",
    "                        for state, target_state in zip(state_batch, target_state_batch)]).mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = quantum_infidelity_batched(outputs, labels) \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 500 == 0:\n",
    "        \n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "    \n",
    "model.eval()\n",
    "\n",
    "q0 = model(torch.tensor([1,0],dtype=torch.complex128, requires_grad=False))\n",
    "q1 = torch.tensor([1,0],dtype=torch.complex128, requires_grad=False)\n",
    "print(q0)\n",
    "\n",
    "for n in range(n_ghz-1):\n",
    "    q0 = torch.kron(q0,q1)\n",
    "    q0 = q0 @ c_not\n",
    "    c_not = torch.kron(i,c_not)\n",
    "    print(q0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e7644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
