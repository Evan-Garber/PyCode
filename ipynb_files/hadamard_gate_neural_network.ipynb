{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91172e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fields = ['input1', 'input2','output1', 'output2']\n",
    "rows = []\n",
    "h_target = torch.tensor([[1.,1.],[1.,-1]] / np.sqrt(2), dtype=torch.complex128)\n",
    "x_target = torch.tensor([[0.,1.],[1.,0.]], dtype=torch.complex128)\n",
    "z_target = torch.tensor([[1.,0.],[0.,-1.]], dtype=torch.complex128)\n",
    "\n",
    "for i in range(8):\n",
    "    phi = np.random.rand() * 2*np.pi\n",
    "    theta = np.random.rand() * np.pi\n",
    "    \n",
    "    input_qubit = torch.tensor([np.sin(theta), np.cos(theta) * np.exp(1j * phi)])\n",
    "    output = x_target @ input_qubit\n",
    "\n",
    "    rows.append([np.sin(theta), np.cos(theta) * np.exp(1j * phi), output[0].item(), output[1].item()])\n",
    "\n",
    "filename = 'qubit.csv'\n",
    "\n",
    "with open(filename, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields)\n",
    "    csvwriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0faa5817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input1</th>\n",
       "      <th>input2</th>\n",
       "      <th>output1</th>\n",
       "      <th>output2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.494649</td>\n",
       "      <td>(0.8673828292772555-0.05448928955634791j)</td>\n",
       "      <td>(0.8673828292772555-0.05448928955634791j)</td>\n",
       "      <td>(0.49464931496831954+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.860763</td>\n",
       "      <td>(0.22507380919089764-0.45654075533033506j)</td>\n",
       "      <td>(0.22507380919089764-0.45654075533033506j)</td>\n",
       "      <td>(0.8607626380940953+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.881012</td>\n",
       "      <td>(0.4536951598302138-0.13408092060643542j)</td>\n",
       "      <td>(0.4536951598302138-0.13408092060643542j)</td>\n",
       "      <td>(0.8810124906469644+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203585</td>\n",
       "      <td>(-0.9609407138940976+0.1874725565198236j)</td>\n",
       "      <td>(-0.9609407138940976+0.1874725565198236j)</td>\n",
       "      <td>(0.20358532592656003+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.925141</td>\n",
       "      <td>(-0.37947100773189796-0.010736631689078211j)</td>\n",
       "      <td>(-0.37947100773189796-0.010736631689078211j)</td>\n",
       "      <td>(0.9251413292199797+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.675454</td>\n",
       "      <td>(0.733924324431402-0.07153187132527726j)</td>\n",
       "      <td>(0.733924324431402-0.07153187132527726j)</td>\n",
       "      <td>(0.6754541267862787+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.312586</td>\n",
       "      <td>(-0.8132635391133761+0.4908080269381142j)</td>\n",
       "      <td>(-0.8132635391133761+0.4908080269381142j)</td>\n",
       "      <td>(0.31258582284214614+0j)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.967378</td>\n",
       "      <td>(0.24857228837494816+0.04891351833133128j)</td>\n",
       "      <td>(0.24857228837494816+0.04891351833133128j)</td>\n",
       "      <td>(0.9673775298075163+0j)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     input1                                        input2  \\\n",
       "0  0.494649     (0.8673828292772555-0.05448928955634791j)   \n",
       "1  0.860763    (0.22507380919089764-0.45654075533033506j)   \n",
       "2  0.881012     (0.4536951598302138-0.13408092060643542j)   \n",
       "3  0.203585     (-0.9609407138940976+0.1874725565198236j)   \n",
       "4  0.925141  (-0.37947100773189796-0.010736631689078211j)   \n",
       "5  0.675454      (0.733924324431402-0.07153187132527726j)   \n",
       "6  0.312586     (-0.8132635391133761+0.4908080269381142j)   \n",
       "7  0.967378    (0.24857228837494816+0.04891351833133128j)   \n",
       "\n",
       "                                        output1                   output2  \n",
       "0     (0.8673828292772555-0.05448928955634791j)  (0.49464931496831954+0j)  \n",
       "1    (0.22507380919089764-0.45654075533033506j)   (0.8607626380940953+0j)  \n",
       "2     (0.4536951598302138-0.13408092060643542j)   (0.8810124906469644+0j)  \n",
       "3     (-0.9609407138940976+0.1874725565198236j)  (0.20358532592656003+0j)  \n",
       "4  (-0.37947100773189796-0.010736631689078211j)   (0.9251413292199797+0j)  \n",
       "5      (0.733924324431402-0.07153187132527726j)   (0.6754541267862787+0j)  \n",
       "6     (-0.8132635391133761+0.4908080269381142j)  (0.31258582284214614+0j)  \n",
       "7    (0.24857228837494816+0.04891351833133128j)   (0.9673775298075163+0j)  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('qubit.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "928ea959",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.9059639054706226\n",
      "epoch: 500, loss: 0.016378814316785784\n",
      "epoch: 1000, loss: 0.000230727751420301\n",
      "epoch: 1500, loss: 0.0002792273051990257\n",
      "tensor([ 9.9997e-01+0.0000e+00j, -3.5649e-06-6.7750e-12j],\n",
      "       dtype=torch.complex128)\n",
      "tensor([9.9997e-01, 3.5649e-06], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "n_ghz = 1\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = pd.read_csv('qubit.csv')\n",
    "        self.df['input1'] = self.df['input1'].astype(complex)\n",
    "        self.df['input2'] = self.df['input2'].astype(complex)\n",
    "        self.df['output1'] = self.df['output1'].astype(complex)\n",
    "        self.df['output2'] = self.df['output2'].astype(complex)\n",
    "\n",
    "        dataset = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "            dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    "            \n",
    "        self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    "        \n",
    "        self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "def elements_to_matrix(matrix_entries: list):\n",
    "    return torch.stack([torch.stack([value for value in row]) for row in matrix_entries]).squeeze()\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        self.θ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.α = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.β = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.ϕ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "      \n",
    "    def forward(self, x):\n",
    "        θ = self.θ\n",
    "        α = self.α\n",
    "        β = self.β\n",
    "        ϕ = self.ϕ\n",
    "\n",
    "#         torch.exp(1j * ϕ / 2) * \n",
    "        \n",
    "#         U = elements_to_matrix(\n",
    "#             [[torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "#              [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]])\n",
    "        U = elements_to_matrix([[α,β],\n",
    "                                [-torch.exp(1j * ϕ) * β.conj(),torch.exp(1j * ϕ) * α.conj()]])\n",
    "    \n",
    "        if len(x.shape) == 1:\n",
    "            return U.matmul(x)\n",
    "        else:\n",
    "            return torch.einsum('ij,bj->bi', U, x)\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "c_not = torch.tensor([[1,0,0,0],\n",
    "                      [0,1,0,0],\n",
    "                      [0,0,0,1],\n",
    "                      [0,0,1,0]], dtype=torch.complex128)\n",
    "\n",
    "def quantum_infidelity_batched(state_batch, target_state_batch):\n",
    "    \n",
    "#     loss = 1 - (abs(torch.dot(torch.conj(target_state_batch).reshape(-1),state_batch.reshape(-1))))**2\n",
    "#     return loss.mean()\n",
    "\n",
    "#     loss = 1 - abs((torch.conj(target_state_batch) * state_batch.sum(dim=-1)))**2\n",
    "#     return loss.mean()\n",
    "\n",
    "    return torch.stack([torch.abs(1 - torch.abs(torch.dot(target_state.conj(), state))**2)\n",
    "                        for state, target_state in zip(state_batch, target_state_batch)]).mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = quantum_infidelity_batched(outputs, labels) \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 500 == 0:\n",
    "        \n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "for i in range(len(n_qhz)):\n",
    "    \n",
    "        \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_data = model(torch.tensor([0.,1.],dtype=torch.complex128, requires_grad=False))\n",
    "    print(out_data) \n",
    "    print(torch.abs(out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f055d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before adding CNOT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "n_qubits = 1\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = pd.read_csv('qubit.csv')\n",
    "        self.df['input1'] = self.df['input1'].astype(complex)\n",
    "        self.df['input2'] = self.df['input2'].astype(complex)\n",
    "        self.df['output1'] = self.df['output1'].astype(complex)\n",
    "        self.df['output2'] = self.df['output2'].astype(complex)\n",
    "\n",
    "        dataset = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "            dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    "            \n",
    "        self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    "        \n",
    "        self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "def elements_to_matrix(matrix_entries: list):\n",
    "    return torch.stack([torch.stack([value for value in row]) for row in matrix_entries]).squeeze()\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        self.θ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.α = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.β = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.ϕ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "      \n",
    "    def forward(self, x):\n",
    "        θ = self.θ\n",
    "        α = self.α\n",
    "        β = self.β\n",
    "        ϕ = self.ϕ\n",
    "\n",
    "#         torch.exp(1j * ϕ / 2) * \n",
    "        \n",
    "#         U = elements_to_matrix(\n",
    "#             [[torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "#              [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]])\n",
    "        U = elements_to_matrix([[α,β],\n",
    "                                [-torch.exp(1j * ϕ) * β.conj(),torch.exp(1j * ϕ) * α.conj()]])\n",
    "    \n",
    "        if len(x.shape) == 1:\n",
    "            return U.matmul(x)\n",
    "        else:\n",
    "            return torch.einsum('ij,bj->bi', U, x)\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "def quantum_infidelity_batched(state_batch, target_state_batch):\n",
    "    \n",
    "#     loss = 1 - (abs(torch.dot(torch.conj(target_state_batch).reshape(-1),state_batch.reshape(-1))))**2\n",
    "#     return loss.mean()\n",
    "\n",
    "#     loss = 1 - abs((torch.conj(target_state_batch) * state_batch.sum(dim=-1)))**2\n",
    "#     return loss.mean()\n",
    "\n",
    "    return torch.stack([torch.abs(1 - torch.abs(torch.dot(target_state.conj(), state))**2)\n",
    "                        for state, target_state in zip(state_batch, target_state_batch)]).mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = quantum_infidelity_batched(outputs, labels) \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 500 == 0:\n",
    "        \n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_data = model(torch.tensor([0.,1.],dtype=torch.complex128, requires_grad=False))\n",
    "    print(out_data) \n",
    "    print(torch.abs(out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "28367d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.9006276419350971\n",
      "epoch: 100, loss: 0.9006276419350971\n",
      "epoch: 200, loss: 0.9006276419350971\n",
      "epoch: 300, loss: 0.9006276419350971\n",
      "epoch: 400, loss: 0.9006276419350971\n",
      "epoch: 500, loss: 0.9006276419350971\n",
      "epoch: 600, loss: 0.9006276419350971\n",
      "epoch: 700, loss: 0.9006276419350971\n",
      "epoch: 800, loss: 0.9006276419350971\n",
      "epoch: 900, loss: 0.9006276419350971\n",
      "tensor([0.1452+0.1932j, 0.9015-0.3589j], dtype=torch.complex128)\n",
      "tensor([0.2417, 0.9704], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Labels have shape [1,2]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = pd.read_csv('qubit.csv')\n",
    "        self.df['input1'] = self.df['input1'].astype(complex)\n",
    "        self.df['input2'] = self.df['input2'].astype(complex)\n",
    "        self.df['output1'] = self.df['output1'].astype(complex)\n",
    "        self.df['output2'] = self.df['output2'].astype(complex)\n",
    "\n",
    "        dataset = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "            dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    "            \n",
    "        self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    "        \n",
    "        self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "def elements_to_matrix(matrix_entries: list):\n",
    "    return torch.stack([torch.stack([value for value in row]) for row in matrix_entries]).squeeze()\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        self.θ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.α = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.β = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.ϕ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "#         self.U = torch.exp(1j * self.ϕ / 2) * torch.tensor(\n",
    "#              [[torch.exp(1j * self.α) * torch.cos(self.θ), torch.exp(1j * self.β) * torch.sin(self.θ)],\n",
    "#              [- torch.exp(-1j * self.β) * torch.sin(self.θ), torch.exp(-1j * self.α) * torch.cos(self.θ)]], \n",
    "#              dtype=torch.complex128, requires_grad = True)\n",
    "#         self.U = Parameter(torch.tensor([[1,1],[1,1]], dtype=torch.complex128, requires_grad=True))\n",
    "#         self.U = Parameter(torch.rand(2,2, dtype=torch.complex128))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        θ = self.θ\n",
    "        α = self.α\n",
    "        β = self.β\n",
    "        ϕ = self.ϕ\n",
    "#         U = self.U\n",
    "        U = torch.exp(1j * ϕ / 2) * elements_to_matrix(\n",
    "            [[torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "             [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]])\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            return U.matmul(x)\n",
    "        else:\n",
    "            return torch.einsum('ij,bj->bi', U, x)\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "# #         loss = 1 - (abs(torch.dot(torch.conj(targets).reshape(-1),inputs.reshape(-1))))**2\n",
    "# #         loss = 1 - (abs(torch.conj(labels).reshape(-1) @ batch.reshape(-1)))**2\n",
    "# #         loss = 1 - (abs((torch.conj(targets) * inputs).sum(dim=-1)))**2\n",
    "#         loss = abs((targets - inputs)**2)\n",
    "#         return loss.mean()\n",
    "\n",
    "# def get_states_1d(batch, num_qubits_in_batch = 1):\n",
    "#         \"\"\"Return the states in a 1d tensor.\"\"\"\n",
    "#         num_states_in_batch = batch.shape[0]\n",
    "#         return torch.reshape(batch, [num_states_in_batch, 2**num_qubits_in_batch])\n",
    "    \n",
    "def CustomLoss(inputs, targets):\n",
    "#     loss = torch.mean((1 - (abs((torch.conj(targets) * inputs).sum(dim=-1)))**2))\n",
    "#     loss = 1 - torch.dot(targets.reshape(-1), conj(inputs.reshape(-1)).abs() ** 2\n",
    "#     losses = [1 - torch.dot(get_states_1d(t), get_states_1d(i)).abs() ** 2 for t, i in zip(targets,inputs)]\n",
    "    \n",
    "#     loss = 1 - torch.dot(get_states_1d(targets), get_states_1d(inputs)).abs() ** 2\n",
    "    loss = 1 - torch.matmul(input.conj(), targets)**2\n",
    "    return torch.mean(loss)\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    "         \n",
    "#         if epoch == 0:\n",
    "#             print(batch)\n",
    "#             print(labels)\n",
    "#             print([(b,l) for b,l in zip(batch,labels)])\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = CustomLoss(outputs,labels)\n",
    "#         loss = criterion(abs(outputs), abs(labels))\n",
    "        \n",
    "#         loss.backward(retain_graph=True)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        \n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_data = model(torch.tensor([0,1],dtype=torch.complex128))\n",
    "    print(out_data)\n",
    "#     print(np.conj(out_data) * out_data) \n",
    "    print(torch.abs(out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        h_target = [[1.,1.],[1.,-1]] / np.sqrt(2)\n",
    "        \n",
    "        input_qubit = []\n",
    "        label_qubit = []\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            phi = np.random.rand() * 2*np.pi\n",
    "            theta = np.random.rand() * np.pi\n",
    "            \n",
    "            test_qubit = [np.sin(theta), np.cos(theta) * np.exp(1j * phi)]\n",
    "            output_qubit = (h_target @ test_qubit)\n",
    "            \n",
    "            input_qubit.append(test_qubit)\n",
    "            label_qubit.append(output_qubit)  \n",
    "        self.dataset = torch.tensor(input_qubit, dtype=torch.complex128)\n",
    "        \n",
    "        self.labels = torch.tensor(label_qubit, dtype=torch.complex128)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "def elements_to_matrix(matrix_entries: list):\n",
    "    return torch.stack([torch.stack([value for value in row]) for row in matrix_entries]).squeeze()\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        self.θ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.α = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.β = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "        self.ϕ = Parameter(torch.rand(1, dtype=torch.float64))\n",
    "#         self.U = torch.exp(1j * self.ϕ / 2) * torch.tensor(\n",
    "#              [[torch.exp(1j * self.α) * torch.cos(self.θ), torch.exp(1j * self.β) * torch.sin(self.θ)],\n",
    "#              [- torch.exp(-1j * self.β) * torch.sin(self.θ), torch.exp(-1j * self.α) * torch.cos(self.θ)]], \n",
    "#              dtype=torch.complex128, requires_grad = True)\n",
    "#         self.U = Parameter(torch.tensor([[1,1],[1,1]], dtype=torch.complex128, requires_grad=True))\n",
    "#         self.U = Parameter(torch.rand(2,2, dtype=torch.complex128))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        θ = self.θ\n",
    "        α = self.α\n",
    "        β = self.β\n",
    "        ϕ = self.ϕ\n",
    "#         U = self.U\n",
    "        U = torch.exp(1j * ϕ / 2) * elements_to_matrix(\n",
    "            [[torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "             [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]])\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            return U.matmul(x)\n",
    "        else:\n",
    "            return torch.einsum('ij,bj->bi', U, x)\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "# #         loss = 1 - (abs(torch.dot(torch.conj(targets).reshape(-1),inputs.reshape(-1))))**2\n",
    "# #         loss = 1 - (abs(torch.conj(labels).reshape(-1) @ batch.reshape(-1)))**2\n",
    "# #         loss = 1 - (abs((torch.conj(targets) * inputs).sum(dim=-1)))**2\n",
    "#         loss = abs((targets - inputs)**2)\n",
    "#         return loss.mean()\n",
    "\n",
    "# def get_states_1d(batch, num_qubits_in_batch = 1):\n",
    "#         \"\"\"Return the states in a 1d tensor.\"\"\"\n",
    "#         num_states_in_batch = batch.shape[0]\n",
    "#         return torch.reshape(batch, [num_states_in_batch, 2**num_qubits_in_batch])\n",
    "    \n",
    "# def CustomLoss(inputs, targets):\n",
    "#     loss = 1 - (abs((torch.conj(targets) * inputs).sum(dim=-1)))**2\n",
    "# #     loss = 1 - torch.dot(torch.conj(targets.reshape(-1)), inputs.reshape(-1)).abs() ** 2\n",
    "# #     losses = [1 - torch.dot(get_states_1d(t), get_states_1d(i)).abs() ** 2 for t, i in zip(targets,inputs)]\n",
    "    \n",
    "# #     loss = 1 - torch.dot(get_states_1d(targets), get_states_1d(inputs)).abs() ** 2\n",
    "# #     loss = 1 - torch.dot(inputs.conj(), targets).abs()**2\n",
    "#     return torch.mean(loss)\n",
    "\n",
    "def quantum_infidelity_batched(state_batch, target_state_batch):\n",
    "    return torch.stack([torch.abs(1 - torch.abs(torch.dot(state.conj(), target_state))**2)\n",
    "                        for state, target_state in zip(state_batch, target_state_batch)]).mean()\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    "         \n",
    "#         if epoch == 0:\n",
    "#             print(batch)\n",
    "#             print(labels)\n",
    "#             print([(b,l) for b,l in zip(batch,labels)])\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = quantum_infidelity_batched(outputs, labels) \n",
    "#         loss = CustomLoss(outputs,labels)\n",
    "#         loss = criterion(abs(outputs), abs(labels))\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        \n",
    "#         print(batch)\n",
    "#         print(outputs)\n",
    "#         print(labels)\n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_data = model(torch.tensor([0,1],dtype=torch.complex128))\n",
    "    print(out_data) \n",
    "    print(torch.abs(out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b7cf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.9994144162415406\n",
      "epoch: 10, loss: 0.9994144162415406\n",
      "epoch: 20, loss: 0.5242399086635763\n",
      "epoch: 30, loss: 0.9803126604981304\n",
      "epoch: 40, loss: 0.5259624880361987\n",
      "epoch: 50, loss: 0.48980423733305134\n",
      "epoch: 60, loss: 0.9994144162415406\n",
      "epoch: 70, loss: 0.6004223450815089\n",
      "epoch: 80, loss: 0.9808413089479036\n",
      "epoch: 90, loss: 0.5242399086635763\n",
      "tensor([ 0.6097+0.1286j, -0.1190-0.5503j], dtype=torch.complex128)\n",
      "tensor([0.6231, 0.5630], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = pd.read_csv('qubit.csv')\n",
    "        self.df['input1'] = self.df['input1'].astype(complex)\n",
    "        self.df['input2'] = self.df['input2'].astype(complex)\n",
    "        self.df['output1'] = self.df['output1'].astype(complex)\n",
    "        self.df['output2'] = self.df['output2'].astype(complex)\n",
    "\n",
    "        dataset = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "            dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    "            \n",
    "        self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    "        \n",
    "        self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=1, shuffle=True, drop_last=False)\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()     \n",
    "        self.linear = nn.Linear(2,2,bias=False, dtype=torch.complex128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "def CustomLoss(inputs, targets):\n",
    "    loss = torch.mean(abs(1 - (abs((torch.conj(targets) * inputs).sum(dim=-1)))**2))\n",
    "    loss.requires_grad=True\n",
    "    return loss\n",
    "\n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         loss = inputs - targets\n",
    "#         return loss.mean()\n",
    "    \n",
    "# loss_fn = CustomLoss()\n",
    "# criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = CustomLoss(batch,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        \n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    out_data = model(torch.tensor([0+0j,1+0j],dtype=torch.complex128))\n",
    "    print(out_data)\n",
    "#     print(np.conj(out_data) * out_data) \n",
    "    print(torch.abs(out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea843f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e8b5655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.3968259962081861\n",
      "epoch: 100, loss: 0.3742005276611748\n",
      "epoch: 200, loss: 0.3506396941683869\n",
      "epoch: 300, loss: 0.32887158645317566\n",
      "epoch: 400, loss: 0.3194107284906814\n",
      "epoch: 500, loss: 0.31146057300145846\n",
      "epoch: 600, loss: 0.3085389139808963\n",
      "epoch: 700, loss: 0.3053625331532731\n",
      "epoch: 800, loss: 0.3021606905542632\n",
      "epoch: 900, loss: 0.29898272701604806\n",
      "tensor([-0.5596-0.0248j,  0.8331-0.3636j], dtype=torch.complex128)\n",
      "tensor([0.5602, 0.9089], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "num_qubits = 8\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "class input_vec_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = pd.read_csv('qubit.csv')\n",
    "        self.df['input1'] = self.df['input1'].astype(complex)\n",
    "        self.df['input2'] = self.df['input2'].astype(complex)\n",
    "        self.df['output1'] = self.df['output1'].astype(complex)\n",
    "        self.df['output2'] = self.df['output2'].astype(complex)\n",
    "\n",
    "        dataset = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "            dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    "            \n",
    "        self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    "        \n",
    "        for i in range(num_qubits):\n",
    "            labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    "        \n",
    "        self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "\n",
    "data_set = input_vec_dataset()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size=8, shuffle=False, drop_last=False)\n",
    "\n",
    "class HModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        self.θ = Parameter(torch.rand(1))\n",
    "        self.α = Parameter(torch.rand(1))\n",
    "        self.β = Parameter(torch.rand(1))\n",
    "        self.ϕ = Parameter(torch.rand(1))\n",
    "        self.U = Parameter(torch.exp(1j * self.ϕ / 2) * torch.tensor(\n",
    "             [[torch.exp(1j * self.α) * torch.cos(self.θ), torch.exp(1j * self.β) * torch.sin(self.θ)],\n",
    "             [- torch.exp(-1j * self.β) * torch.sin(self.θ), torch.exp(-1j * self.α) * torch.cos(self.θ)]], \n",
    "             dtype=torch.complex128, requires_grad = True))\n",
    "#         self.U = Parameter(torch.tensor([[1,1],[1,1]], dtype=torch.complex128, requires_grad=True))\n",
    "#         self.U = Parameter(torch.rand(2,2, dtype=torch.complex128))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        θ = self.θ\n",
    "        α = self.α\n",
    "        β = self.β\n",
    "        ϕ = self.ϕ\n",
    "        U = self.U\n",
    "#         U = torch.exp(1j * ϕ / 2) * torch.tensor(\n",
    "#             [[torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "#              [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]], \n",
    "#             dtype=torch.complex128)\n",
    "        \n",
    "        x = torch.matmul(x,U)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = HModel()\n",
    "\n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "# #         loss = 1 - (abs(torch.dot(torch.conj(targets).reshape(-1),inputs.reshape(-1))))**2\n",
    "# #         loss = 1 - (abs(torch.conj(labels).reshape(-1) @ batch.reshape(-1)))**2\n",
    "# #         loss = 1 - (abs((torch.conj(targets) * inputs).sum(dim=-1)))**2\n",
    "#         loss = abs((targets - inputs)**2)\n",
    "#         return loss.mean()\n",
    "    \n",
    "def CustomLoss(inputs, targets):\n",
    "    loss = torch.mean(torch.abs(1 - (abs((torch.conj(targets) * inputs).sum(dim=-1)))**2))\n",
    "    return loss\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch, labels in data_loader:\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        loss = CustomLoss(outputs,labels)\n",
    "#         loss = criterion(abs(outputs), abs(labels))\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        \n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_data = model(torch.tensor([0,1],dtype=torch.complex128))\n",
    "    print(out_data)\n",
    "#     print(np.conj(out_data) * out_data) \n",
    "    print(torch.abs(out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3da49e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.9830+0.0921j, 0.1470+0.5384j],\n",
      "        [0.2620+0.0375j, 0.8106+0.5605j]], dtype=torch.complex128,\n",
      "       requires_grad=True)\n",
      "epoch: 0, loss: 0.9360205268486045\n",
      "\n",
      "targets: tensor([0.7071+0.j, 0.7071+0.j], dtype=torch.complex128)\n",
      "inputs:  tensor([0.9930+0.0821j, 0.1570+0.5284j], dtype=torch.complex128,\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "loss: 0.15228676286305698\n",
      "Parameter containing:\n",
      "tensor([[1.1225-0.2218j, 0.2865+0.2245j],\n",
      "        [0.2620+0.0375j, 0.8106+0.5605j]], dtype=torch.complex128,\n",
      "       requires_grad=True)\n",
      "epoch: 100, loss: 0.010046450235292022\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[1.1255-0.2236j, 0.2895+0.2228j],\n",
      "        [0.2620+0.0375j, 0.8106+0.5605j]], dtype=torch.complex128,\n",
      "       requires_grad=True)\n",
      "epoch: 200, loss: 0.0036788942403149743\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[1.1284-0.2241j, 0.2925+0.2222j],\n",
      "        [0.2620+0.0375j, 0.8106+0.5605j]], dtype=torch.complex128,\n",
      "       requires_grad=True)\n",
      "epoch: 300, loss: 0.007440093690240433\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[1.1251-0.2240j, 0.2891+0.2223j],\n",
      "        [0.2620+0.0375j, 0.8106+0.5605j]], dtype=torch.complex128,\n",
      "       requires_grad=True)\n",
      "epoch: 400, loss: 0.0064459148712229615\n",
      "\n",
      "out_data: tensor([1.1251-0.2208j, 0.2891+0.2255j], dtype=torch.complex128)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    " \n",
    "num_qubits = 1\n",
    " \n",
    "num_epochs = 500\n",
    " \n",
    "learning_rate = 1e-2\n",
    " \n",
    "# class input_vec_dataset(Dataset):\n",
    "#     def __init__(self):\n",
    " \n",
    "#         self.df = pd.read_csv('qubit.csv')\n",
    "#         self.df['input1'] = self.df['input1'].astype(complex)\n",
    "#         self.df['input2'] = self.df['input2'].astype(complex)\n",
    "#         self.df['output1'] = self.df['output1'].astype(complex)\n",
    "#         self.df['output2'] = self.df['output2'].astype(complex)\n",
    " \n",
    "#         dataset = []\n",
    "#         labels = []\n",
    " \n",
    "#         for i in range(num_qubits):\n",
    "#             dataset.append([self.df['input1'][i],self.df['input2'][i]])\n",
    " \n",
    "#         self.dataset = torch.tensor(dataset, dtype=torch.complex128)\n",
    " \n",
    "#         for i in range(num_qubits):\n",
    "#             labels.append([self.df['output1'][i],self.df['output2'][i]])\n",
    " \n",
    "#         self.labels = torch.tensor(labels, dtype=torch.complex128)\n",
    " \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    " \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.dataset[idx],self.labels[idx]\n",
    " \n",
    "# data_set = input_vec_dataset()\n",
    " \n",
    "# data_loader = torch.utils.data.DataLoader(data_set, batch_size=num_qubits, shuffle=False, drop_last=False)\n",
    " \n",
    "class HModel(torch.nn.Module):\n",
    " \n",
    "    def __init__(self):\n",
    "        super(HModel, self).__init__()\n",
    "        # self.θ = Parameter(torch.rand(1, dtype=torch.float64), requires_grad = True)\n",
    "        # self.α = Parameter(torch.rand(1, dtype=torch.float64), requires_grad = True)\n",
    "        # self.β = Parameter(torch.rand(1, dtype=torch.float64), requires_grad = True)\n",
    "        # self.ϕ = Parameter(torch.rand(1, dtype=torch.float64), requires_grad = True)\n",
    "        # self.U = Parameter(torch.exp(1j * self.ϕ / 2) * torch.tensor(\n",
    "        #      [[torch.exp(1j * self.α) * torch.cos(self.θ), torch.exp(1j * self.β) * torch.sin(self.θ)],\n",
    "        #      [- torch.exp(-1j * self.β) * torch.sin(self.θ), torch.exp(-1j * self.α) * torch.cos(self.θ)]], \n",
    "        #      dtype=torch.complex128))\n",
    "        # self.U = Parameter(torch.tensor([[1,1],[1,1]], dtype=torch.complex128, requires_grad=True))\n",
    "        self.U = Parameter(torch.rand(2,2, dtype=torch.complex128), requires_grad=True)\n",
    " \n",
    "    def forward(self, x):\n",
    "#         θ = self.θ\n",
    "#         α = self.α\n",
    "#         β = self.β\n",
    "#         ϕ = self.ϕ\n",
    " \n",
    "        # U = torch.exp(1j * self.ϕ / 2) * torch.tensor(\n",
    "        #      [[torch.exp(1j * self.α) * torch.cos(self.θ), torch.exp(1j * self.β) * torch.sin(self.θ)],\n",
    "        #      [- torch.exp(-1j * self.β) * torch.sin(self.θ), torch.exp(-1j * self.α) * torch.cos(self.θ)]], \n",
    "        #      dtype=torch.complex128)\n",
    "        # if epoch == 100:\n",
    "        #     print(U)\n",
    "        #     print()\n",
    " \n",
    "        U = self.U\n",
    "#         U = torch.exp(1j * ϕ / 2) * torch.tensor(\n",
    "#             [[torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "#              [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]], \n",
    "#             dtype=torch.complex128)\n",
    " \n",
    "        x = torch.matmul(x,U)\n",
    " \n",
    "        return x\n",
    " \n",
    "model = HModel()\n",
    " \n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomLoss, self).__init__()\n",
    " \n",
    "#     def forward(self, inputs, targets):\n",
    "# #         loss = 1 - (abs(torch.dot(torch.conj(targets).reshape(-1),inputs.reshape(-1))))**2\n",
    "# #         loss = 1 - (abs(torch.conj(labels).reshape(-1) @ batch.reshape(-1)))**2\n",
    "# #         loss = 1 - (abs((torch.conj(targets) * inputs).sum(dim=-1)))**2\n",
    "#         loss = abs((targets - inputs)**2)\n",
    "#         return loss.mean()\n",
    " \n",
    " \n",
    "# def get_states_1d(batch, num_qubits_in_batch = 1):\n",
    "#         \"\"\"Return the states in a 1d tensor.\"\"\"\n",
    "#         num_states_in_batch = batch.shape[0]\n",
    "#         return torch.reshape(batch, [num_states_in_batch, 2**num_qubits_in_batch])\n",
    " \n",
    "def quantum_infidelity(inputs, targets):\n",
    "    # loss = torch.mean(1 - torch.abs((torch.conj(targets) * inputs).sum(dim=-1))**2)\n",
    "    # if epoch == 1:\n",
    "    #     print(f\"inputs:  {inputs}\")\n",
    "    #     print(f\"targets: {targets}\")\n",
    "    #     print(torch.conj(targets) * inputs)\n",
    "    #     print(f\"sum {(torch.conj(targets) * inputs).sum(dim=-1)}\")\n",
    "    #     print(torch.abs(1 - (torch.abs((torch.conj(targets) * inputs).sum(dim=-1)))**2))\n",
    "    #     print(f\"loss {torch.mean(torch.abs(1 - (torch.abs((torch.conj(targets) * inputs).sum(dim=-1)))**2))}\")\n",
    "    #     print(\"\\n\\n\\n\")\n",
    " \n",
    " \n",
    "    loss = torch.abs(1 - ((torch.conj(targets) * inputs).sum(dim=-1))**2)\n",
    " \n",
    "    # loss = torch.abs(1 - torch.dot(targets, inputs).abs() ** 2)\n",
    " \n",
    "    if epoch == 2:\n",
    "        # print(1 - torch.dot(targets, inputs))\n",
    "        print(f\"targets: {targets}\")\n",
    "        print(f\"inputs:  {inputs}\")\n",
    "        print(f\"loss: {1 - torch.dot(targets, inputs).abs() ** 2}\")\n",
    " \n",
    "    # losses = [1 - torch.dot(get_states_1d(t), get_states_1d(i)).abs() ** 2 for t, i in zip(targets,inputs)]\n",
    " \n",
    "#     loss = 1 - torch.dot(get_states_1d(targets), get_states_1d(inputs)).abs() ** 2\n",
    " \n",
    "    return loss # torch.mean(losses)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "#     for batch, labels in data_loader:\n",
    "#         if epoch == 0:\n",
    "#             print(batch)\n",
    "#             print(labels)\n",
    "#             print([(b,l) for b,l in zip(batch,labels)])\n",
    " \n",
    "    inputs = torch.tensor([1,0], dtype=torch.complex128, requires_grad=False)\n",
    "    outputs = model(inputs)\n",
    "    labels = torch.tensor([1,1] / np.sqrt(2), dtype=torch.complex128, requires_grad=False)\n",
    " \n",
    "    # outputs = model(batch)\n",
    "    loss = quantum_infidelity(outputs,labels)\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    " \n",
    "    if epoch % 100 == 0:\n",
    "        print(model.U)\n",
    "        print(f'epoch: {epoch}, loss: {loss}')\n",
    "        print()\n",
    " \n",
    "model.eval()\n",
    " \n",
    "with torch.no_grad():\n",
    "    out_data = model(torch.tensor([1, 0],dtype=torch.complex128, requires_grad=False))\n",
    "    print(f\"out_data: {out_data}\")\n",
    "#     print(np.conj(out_data) * out_data) \n",
    "    # print(torch.abs(out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37f1ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "q0 = torch.tensor([1,0], dtype=torch.complex128, requires_grad=False)\n",
    "H = torch.tensor([[1,1],[1,-1]] / np.sqrt(2), dtype=torch.complex128, requires_grad=False)\n",
    "q0_target = H.matmul(q0)\n",
    "print(q0.shape)\n",
    "print(q0_target.shape)\n",
    "print(model(q0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "750995ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hey Ben, I have a question. Is it necessary to have a csv file for the input qubits and their labels? Or can I simply create the qubits and labels within the _init _ section of my dataset itself? I'm running into the problem that the label qubits have shape [1,2], so they can't be used for torch.dot. I believe this is due to the way I unpack the csv file in order to create the input qubits and their corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8ed121de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.tensor([1,2])\n",
    "w = torch.tensor([3,4])\n",
    "print(q)\n",
    "torch.dot(q,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0162a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "U:    tensor([[ 0.5992+0.7851j,  0.0245+0.1548j],\n",
      "        [-0.1310+0.0860j,  0.9840-0.0843j]], dtype=torch.complex128,\n",
      "       grad_fn=<MulBackward0>)\n",
      "Loss: (0.5681979136614455-0.2294774991857891j)\n",
      "\n",
      "U:    tensor([[-0.0528+7.0461e-01j, -0.7076-3.3870e-04j],\n",
      "        [-0.0521+7.0571e-01j,  0.7066-4.1319e-04j]], dtype=torch.complex128,\n",
      "       grad_fn=<MulBackward0>)\n",
      "Loss: (8.331665358740636e-07-0.00010534926150468421j)\n",
      "\n",
      "U:    tensor([[-0.0523+7.0517e-01j, -0.7071-1.5497e-08j],\n",
      "        [-0.0523+7.0517e-01j,  0.7071-1.6396e-08j]], dtype=torch.complex128,\n",
      "       grad_fn=<MulBackward0>)\n",
      "Loss: (4.440892098500626e-16-1.2724436724166338e-09j)\n",
      "\n",
      "U:    tensor([[-0.0523+7.0517e-01j, -0.7071-5.9919e-13j],\n",
      "        [-0.0523+7.0517e-01j,  0.7071-2.9959e-13j]], dtype=torch.complex128,\n",
      "       grad_fn=<MulBackward0>)\n",
      "Loss: 4.2368944997581665e-13j\n",
      "\n",
      "U:    tensor([[-0.0523+7.0517e-01j, -0.7071-2.7756e-16j],\n",
      "        [-0.0523+7.0517e-01j,  0.7071-1.1102e-16j]], dtype=torch.complex128,\n",
      "       grad_fn=<MulBackward0>)\n",
      "Loss: (2.220446049250313e-16+2.355138688025662e-16j)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "q0 = torch.tensor([0,1], dtype=torch.complex128, requires_grad=False)\n",
    "\n",
    "α = torch.rand(1, dtype=torch.float64, requires_grad = True)\n",
    "β = torch.rand(1, dtype=torch.float64, requires_grad = True)\n",
    "θ = torch.rand(1, dtype=torch.float64, requires_grad = True)\n",
    "ϕ = torch.rand(1, dtype=torch.float64, requires_grad = True)\n",
    "\n",
    "# U = torch.rand(2,2, dtype=torch.complex128, requires_grad=True)\n",
    "H = torch.tensor([[1,1],[1,-1]] / np.sqrt(2), dtype=torch.complex128, requires_grad=False)\n",
    "\n",
    "def elements_to_matrix(matrix_entries: list):\n",
    "    return torch.stack([torch.stack([value for value in row]) for row in matrix_entries]).squeeze()\n",
    "    \n",
    "def quantum_infidelity(state, target_state):\n",
    "    return 1 - torch.dot(state.conj(), target_state)**2\n",
    "\n",
    "optimizer = torch.optim.Adam([α, β, θ, ϕ], lr=learning_rate)\n",
    "\n",
    "print(elements_to_matrix([[torch.tensor(1), torch.tensor(2)],[torch.tensor(3), torch.tensor(4)]]))\n",
    "\n",
    "for epoch in range(1000):\n",
    "    U = torch.exp(1j * ϕ / 2) * elements_to_matrix([\n",
    "        [torch.exp(1j * α) * torch.cos(θ), torch.exp(1j * β) * torch.sin(θ)],\n",
    "        [- torch.exp(-1j * β) * torch.sin(θ), torch.exp(-1j * α) * torch.cos(θ)]\n",
    "    ])\n",
    "    q0_out = U.matmul(q0)\n",
    "    q0_target = H.matmul(q0)\n",
    "    loss = quantum_infidelity(q0_out, q0_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"U:    {U}\")\n",
    "        print(f\"Loss: {loss}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7a2cb12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 1.], dtype=torch.float64)\n",
      "tensor([1., 0., 0., 1., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "tensor([1., 0., 0., 1., 0., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,1], dtype=torch.float64)\n",
    "b = torch.tensor([1,0], dtype=torch.float64)\n",
    "c = torch.tensor([1,0], dtype=torch.float64)\n",
    "\n",
    "c_not = torch.tensor([[1,0,0,0],\n",
    "                      [0,1,0,0],\n",
    "                      [0,0,0,1],\n",
    "                      [0,0,1,0]], dtype=torch.float64)\n",
    "\n",
    "i = torch.tensor([[1,0],\n",
    "                  [0,1]], dtype=torch.float64)\n",
    "\n",
    "g = torch.tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                  [0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "                  [0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "                  [0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "                  [0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "                  [0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "                  [0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "                  [0., 0., 0., 0., 0., 0., 1., 0.]], dtype=torch.float64)\n",
    "\n",
    "d = torch.kron(a,b)\n",
    "\n",
    "e = d @ c_not\n",
    "\n",
    "f = torch.kron(c,e)\n",
    "\n",
    "h = g @ f\n",
    "\n",
    "print(e)\n",
    "print(f)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7ef6eb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  1,  1],\n",
       "        [ 0,  0,  1, -1],\n",
       "        [ 1,  1,  0,  0],\n",
       "        [ 1, -1,  0,  0]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0,1],[1,0]])\n",
    "h = torch.tensor([[1,1],[1,-1]])\n",
    "i = torch.tensor([[1,0],[0,1]])\n",
    "\n",
    "v = torch.kron(x,h)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad87b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
